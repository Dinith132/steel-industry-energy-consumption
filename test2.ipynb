{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dinit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.30982]]\n"
     ]
    }
   ],
   "source": [
    "# univariate bidirectional lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "# split a univariate sequence\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10],\n",
       "        [20],\n",
       "        [30]],\n",
       "\n",
       "       [[20],\n",
       "        [30],\n",
       "        [40]],\n",
       "\n",
       "       [[30],\n",
       "        [40],\n",
       "        [50]],\n",
       "\n",
       "       [[40],\n",
       "        [50],\n",
       "        [60]],\n",
       "\n",
       "       [[50],\n",
       "        [60],\n",
       "        [70]],\n",
       "\n",
       "       [[60],\n",
       "        [70],\n",
       "        [80]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dinit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1782\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0998\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1008\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0892\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0850\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0791\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0846\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0749\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0822\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0799\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0768\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0847\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0786\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0772\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0879\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0786\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0780\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0763\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0764\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0731\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AE5CE9FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 182ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AE5CE9FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predicted values: [0.53096443 0.5601973  0.54770195 0.58249974 0.59233475 0.5608948\n",
      " 0.6043304  0.5895189  0.5439857  0.6152338  0.535155   0.52658355\n",
      " 0.5619463  0.5422111  0.56119984 0.42494074 0.6073916  0.56100285\n",
      " 0.6059102  0.58223337 0.505952   0.5334827  0.5568124  0.6296102\n",
      " 0.6225586 ]\n",
      "Actual values: [0.51340878 0.64477523 0.05803056 0.69573053 0.70720087]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, input_shape=(timesteps, features)),\n",
    "    Dense(1) \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(, y_train, epochs=20, batch_size=8, verbose=1)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print first few predictions\n",
    "print(\"Predicted values:\", y_pred[:5].flatten())\n",
    "print(\"Actual values:\", y_test[:5].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.72999831, 0.184512  , 0.34663969, ..., 0.11654669,\n",
       "         0.70956772, 0.23034416],\n",
       "        [0.41447673, 0.03286273, 0.13590738, ..., 0.97218297,\n",
       "         0.59907151, 0.24171373],\n",
       "        [0.32765503, 0.31983592, 0.32649433, ..., 0.02213916,\n",
       "         0.21883468, 0.1074714 ],\n",
       "        [0.78544503, 0.11249796, 0.72422824, ..., 0.23576151,\n",
       "         0.89468117, 0.04525831],\n",
       "        [0.20181961, 0.75204693, 0.70937793, ..., 0.19571341,\n",
       "         0.51606182, 0.9102272 ]],\n",
       "\n",
       "       [[0.04354658, 0.76580409, 0.57247323, ..., 0.14364737,\n",
       "         0.91288175, 0.66895308],\n",
       "        [0.43798602, 0.57325957, 0.74561391, ..., 0.63966427,\n",
       "         0.4157279 , 0.59459845],\n",
       "        [0.4199858 , 0.15299579, 0.46942675, ..., 0.41468771,\n",
       "         0.78629393, 0.41285927],\n",
       "        [0.69653079, 0.50721368, 0.24122313, ..., 0.06706563,\n",
       "         0.50293394, 0.49956216],\n",
       "        [0.7069009 , 0.74475069, 0.72779149, ..., 0.59721269,\n",
       "         0.94724788, 0.19289374]],\n",
       "\n",
       "       [[0.9057721 , 0.05345551, 0.30838921, ..., 0.82855141,\n",
       "         0.65206582, 0.45841383],\n",
       "        [0.93295604, 0.83074655, 0.49931515, ..., 0.4620846 ,\n",
       "         0.07812143, 0.35734869],\n",
       "        [0.69582852, 0.95321603, 0.96306411, ..., 0.07065879,\n",
       "         0.28468569, 0.66836335],\n",
       "        [0.31031013, 0.04984687, 0.83187041, ..., 0.17801595,\n",
       "         0.77430012, 0.32168405],\n",
       "        [0.80469579, 0.173846  , 0.49287918, ..., 0.22852515,\n",
       "         0.4937192 , 0.88399255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.91400004, 0.90147758, 0.51876195, ..., 0.10629486,\n",
       "         0.48735437, 0.63439726],\n",
       "        [0.10368029, 0.5196071 , 0.57752129, ..., 0.98312641,\n",
       "         0.21749318, 0.98590383],\n",
       "        [0.71434253, 0.82608207, 0.08964127, ..., 0.10527306,\n",
       "         0.56237278, 0.00607062],\n",
       "        [0.71268282, 0.19719269, 0.12958454, ..., 0.00846442,\n",
       "         0.96841486, 0.5921456 ],\n",
       "        [0.71989964, 0.65138317, 0.14436456, ..., 0.55888111,\n",
       "         0.11907343, 0.03443597]],\n",
       "\n",
       "       [[0.79823431, 0.72592244, 0.0818189 , ..., 0.25398745,\n",
       "         0.48780299, 0.23645958],\n",
       "        [0.62280014, 0.11541424, 0.12724317, ..., 0.87930105,\n",
       "         0.74032848, 0.46172292],\n",
       "        [0.98435599, 0.4562561 , 0.11245067, ..., 0.50355215,\n",
       "         0.55871295, 0.2400959 ],\n",
       "        [0.55685876, 0.75606111, 0.26650984, ..., 0.87119644,\n",
       "         0.17004989, 0.07357964],\n",
       "        [0.1193835 , 0.16101833, 0.53308266, ..., 0.9268722 ,\n",
       "         0.18115261, 0.91247499]],\n",
       "\n",
       "       [[0.05622743, 0.94821945, 0.20079826, ..., 0.97480777,\n",
       "         0.60642649, 0.72148541],\n",
       "        [0.08800525, 0.62781243, 0.45384978, ..., 0.25937253,\n",
       "         0.37612243, 0.30406354],\n",
       "        [0.25387884, 0.04966871, 0.83674633, ..., 0.56300476,\n",
       "         0.50244179, 0.49123457],\n",
       "        [0.57450433, 0.28705661, 0.66294927, ..., 0.39361034,\n",
       "         0.84761779, 0.92025825],\n",
       "        [0.53686627, 0.21495245, 0.91579216, ..., 0.92327409,\n",
       "         0.63728565, 0.95183855]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09237084],\n",
       "       [0.06075373],\n",
       "       [0.60419195],\n",
       "       [0.96611632],\n",
       "       [0.50272131],\n",
       "       [0.05151523],\n",
       "       [0.22121866],\n",
       "       [0.24479291],\n",
       "       [0.33785713],\n",
       "       [0.04360338],\n",
       "       [0.9048927 ],\n",
       "       [0.51556394],\n",
       "       [0.67093079],\n",
       "       [0.06717284],\n",
       "       [0.05411547],\n",
       "       [0.58646777],\n",
       "       [0.88729601],\n",
       "       [0.528482  ],\n",
       "       [0.87781917],\n",
       "       [0.81099127],\n",
       "       [0.07585379],\n",
       "       [0.20195747],\n",
       "       [0.65771067],\n",
       "       [0.44350138],\n",
       "       [0.29069981],\n",
       "       [0.41863386],\n",
       "       [0.44519965],\n",
       "       [0.91055225],\n",
       "       [0.10345828],\n",
       "       [0.27973075],\n",
       "       [0.53756482],\n",
       "       [0.69828693],\n",
       "       [0.63063729],\n",
       "       [0.94164274],\n",
       "       [0.79560892],\n",
       "       [0.55707684],\n",
       "       [0.21468269],\n",
       "       [0.45087423],\n",
       "       [0.69338224],\n",
       "       [0.83796217],\n",
       "       [0.95973622],\n",
       "       [0.60545277],\n",
       "       [0.52691937],\n",
       "       [0.01769192],\n",
       "       [0.07209907],\n",
       "       [0.156105  ],\n",
       "       [0.05475993],\n",
       "       [0.40086324],\n",
       "       [0.33687855],\n",
       "       [0.63735416],\n",
       "       [0.53566511],\n",
       "       [0.59597076],\n",
       "       [0.4719772 ],\n",
       "       [0.27773715],\n",
       "       [0.95282531],\n",
       "       [0.08640916],\n",
       "       [0.938891  ],\n",
       "       [0.31123805],\n",
       "       [0.51266145],\n",
       "       [0.10884232],\n",
       "       [0.07093107],\n",
       "       [0.42954545],\n",
       "       [0.74169738],\n",
       "       [0.26936478],\n",
       "       [0.56234789],\n",
       "       [0.27865034],\n",
       "       [0.38861306],\n",
       "       [0.90644632],\n",
       "       [0.85583963],\n",
       "       [0.51601754],\n",
       "       [0.23810563],\n",
       "       [0.19318391],\n",
       "       [0.95408735],\n",
       "       [0.2027166 ],\n",
       "       [0.80816831],\n",
       "       [0.36897021],\n",
       "       [0.98565985],\n",
       "       [0.36815574],\n",
       "       [0.35151662],\n",
       "       [0.63746363],\n",
       "       [0.96151681],\n",
       "       [0.97620961],\n",
       "       [0.70720615],\n",
       "       [0.75460637],\n",
       "       [0.56340218],\n",
       "       [0.75577574],\n",
       "       [0.85977965],\n",
       "       [0.4860953 ],\n",
       "       [0.61986226],\n",
       "       [0.5373167 ],\n",
       "       [0.56247061],\n",
       "       [0.89004791],\n",
       "       [0.35561683],\n",
       "       [0.20818403],\n",
       "       [0.45628011],\n",
       "       [0.20808519],\n",
       "       [0.49840043],\n",
       "       [0.90350811],\n",
       "       [0.61743352],\n",
       "       [0.85543285],\n",
       "       [0.16619092],\n",
       "       [0.90087359],\n",
       "       [0.60178716],\n",
       "       [0.03175979],\n",
       "       [0.63311896],\n",
       "       [0.95139696],\n",
       "       [0.079167  ],\n",
       "       [0.58245613],\n",
       "       [0.9154786 ],\n",
       "       [0.00969517],\n",
       "       [0.63091293],\n",
       "       [0.47490229],\n",
       "       [0.34698932],\n",
       "       [0.62753649],\n",
       "       [0.05270209],\n",
       "       [0.75306305],\n",
       "       [0.00447948],\n",
       "       [0.30693574],\n",
       "       [0.50395864],\n",
       "       [0.37702225],\n",
       "       [0.30747594],\n",
       "       [0.83083515],\n",
       "       [0.93483257],\n",
       "       [0.87371268],\n",
       "       [0.52243131],\n",
       "       [0.88416418],\n",
       "       [0.92086157],\n",
       "       [0.4092384 ],\n",
       "       [0.28344586],\n",
       "       [0.09826521],\n",
       "       [0.47909192],\n",
       "       [0.94629692],\n",
       "       [0.56090753],\n",
       "       [0.6675621 ],\n",
       "       [0.11288382],\n",
       "       [0.14351966],\n",
       "       [0.46447326],\n",
       "       [0.9512648 ],\n",
       "       [0.36078464],\n",
       "       [0.87748404],\n",
       "       [0.12028859],\n",
       "       [0.61701721],\n",
       "       [0.60061298],\n",
       "       [0.85987475],\n",
       "       [0.0033167 ],\n",
       "       [0.18993167],\n",
       "       [0.21968763],\n",
       "       [0.94996609],\n",
       "       [0.15369366],\n",
       "       [0.03696235],\n",
       "       [0.53446149],\n",
       "       [0.52855846],\n",
       "       [0.84404697],\n",
       "       [0.94885965],\n",
       "       [0.53854544],\n",
       "       [0.92999418],\n",
       "       [0.72515915],\n",
       "       [0.26455913],\n",
       "       [0.92842071],\n",
       "       [0.66744318],\n",
       "       [0.41409593],\n",
       "       [0.68024208],\n",
       "       [0.40131283],\n",
       "       [0.11418377],\n",
       "       [0.71897875],\n",
       "       [0.82408761],\n",
       "       [0.7288062 ],\n",
       "       [0.22911467],\n",
       "       [0.95663501],\n",
       "       [0.02525456],\n",
       "       [0.3326853 ],\n",
       "       [0.26009243],\n",
       "       [0.04812623],\n",
       "       [0.40572969],\n",
       "       [0.62311484],\n",
       "       [0.88255286],\n",
       "       [0.98968972],\n",
       "       [0.80808959],\n",
       "       [0.46868052],\n",
       "       [0.72370443],\n",
       "       [0.84362379],\n",
       "       [0.4653072 ],\n",
       "       [0.78505443],\n",
       "       [0.48265242],\n",
       "       [0.38649599],\n",
       "       [0.52609084],\n",
       "       [0.67585474],\n",
       "       [0.72688206],\n",
       "       [0.35673627],\n",
       "       [0.57132025],\n",
       "       [0.90987087],\n",
       "       [0.9763129 ],\n",
       "       [0.22724809],\n",
       "       [0.19283117],\n",
       "       [0.67668563],\n",
       "       [0.41468389],\n",
       "       [0.02422234],\n",
       "       [0.34686187],\n",
       "       [0.80674474],\n",
       "       [0.79742693],\n",
       "       [0.49468515],\n",
       "       [0.47257793],\n",
       "       [0.57950744],\n",
       "       [0.21939671],\n",
       "       [0.52604362],\n",
       "       [0.01104636],\n",
       "       [0.47555955],\n",
       "       [0.33605105],\n",
       "       [0.67486961],\n",
       "       [0.63835358],\n",
       "       [0.38083588],\n",
       "       [0.00727298],\n",
       "       [0.25227363],\n",
       "       [0.63334785],\n",
       "       [0.07237334],\n",
       "       [0.10764242],\n",
       "       [0.64703502],\n",
       "       [0.7557674 ],\n",
       "       [0.32154971],\n",
       "       [0.48967307],\n",
       "       [0.94571136],\n",
       "       [0.94554356],\n",
       "       [0.30750277],\n",
       "       [0.80998788],\n",
       "       [0.59394405],\n",
       "       [0.97548449],\n",
       "       [0.47761357],\n",
       "       [0.77470687],\n",
       "       [0.76813713],\n",
       "       [0.49743287],\n",
       "       [0.79563011],\n",
       "       [0.26104033],\n",
       "       [0.12225304],\n",
       "       [0.42516374],\n",
       "       [0.94363066],\n",
       "       [0.50074461],\n",
       "       [0.63525914],\n",
       "       [0.62416246],\n",
       "       [0.13257287],\n",
       "       [0.39029688],\n",
       "       [0.70693897],\n",
       "       [0.9719901 ],\n",
       "       [0.16801774],\n",
       "       [0.41044189],\n",
       "       [0.0950938 ],\n",
       "       [0.24237646],\n",
       "       [0.37675934],\n",
       "       [0.54064325],\n",
       "       [0.23056688],\n",
       "       [0.22756266],\n",
       "       [0.77782998],\n",
       "       [0.96264166],\n",
       "       [0.32361297],\n",
       "       [0.68799202],\n",
       "       [0.25808046],\n",
       "       [0.92795225],\n",
       "       [0.67290236],\n",
       "       [0.80267685],\n",
       "       [0.04935404],\n",
       "       [0.59850597],\n",
       "       [0.52761233],\n",
       "       [0.29299691],\n",
       "       [0.26242999],\n",
       "       [0.5800248 ],\n",
       "       [0.70648564],\n",
       "       [0.03317276],\n",
       "       [0.56740158],\n",
       "       [0.17045953],\n",
       "       [0.14036273],\n",
       "       [0.62311949],\n",
       "       [0.31087012],\n",
       "       [0.44088023],\n",
       "       [0.35999881],\n",
       "       [0.64833632],\n",
       "       [0.47011977],\n",
       "       [0.96345866],\n",
       "       [0.35238276],\n",
       "       [0.74028434],\n",
       "       [0.25729257],\n",
       "       [0.50875309],\n",
       "       [0.55256975],\n",
       "       [0.32910928],\n",
       "       [0.47178053],\n",
       "       [0.27927864],\n",
       "       [0.00600078],\n",
       "       [0.2156483 ],\n",
       "       [0.70006417],\n",
       "       [0.77096302],\n",
       "       [0.28020287],\n",
       "       [0.408043  ],\n",
       "       [0.03736566],\n",
       "       [0.2370629 ],\n",
       "       [0.77921928],\n",
       "       [0.60598627],\n",
       "       [0.20577052],\n",
       "       [0.257442  ],\n",
       "       [0.81680202],\n",
       "       [0.2090842 ],\n",
       "       [0.33415773],\n",
       "       [0.66435869],\n",
       "       [0.56111239],\n",
       "       [0.46087021],\n",
       "       [0.89974109],\n",
       "       [0.21971046],\n",
       "       [0.74141989],\n",
       "       [0.15906384],\n",
       "       [0.61573383],\n",
       "       [0.87062356],\n",
       "       [0.45790179],\n",
       "       [0.72429451],\n",
       "       [0.82039335],\n",
       "       [0.13602642],\n",
       "       [0.7058269 ],\n",
       "       [0.91984102],\n",
       "       [0.57637061],\n",
       "       [0.18612224],\n",
       "       [0.12694253],\n",
       "       [0.25973533],\n",
       "       [0.10351327],\n",
       "       [0.51339407],\n",
       "       [0.06970468],\n",
       "       [0.22340462],\n",
       "       [0.43991044],\n",
       "       [0.47391475],\n",
       "       [0.56539849],\n",
       "       [0.70561496],\n",
       "       [0.04722864],\n",
       "       [0.08062095],\n",
       "       [0.06085915],\n",
       "       [0.42315032],\n",
       "       [0.04184794],\n",
       "       [0.74608127],\n",
       "       [0.22300018],\n",
       "       [0.93921249],\n",
       "       [0.48640376],\n",
       "       [0.44632575],\n",
       "       [0.78053869],\n",
       "       [0.71315398],\n",
       "       [0.60730904],\n",
       "       [0.93514671],\n",
       "       [0.42225926],\n",
       "       [0.80853168],\n",
       "       [0.38009295],\n",
       "       [0.83834567],\n",
       "       [0.25947463],\n",
       "       [0.28926707],\n",
       "       [0.67786953],\n",
       "       [0.21271329],\n",
       "       [0.22915419],\n",
       "       [0.30526243],\n",
       "       [0.67892877],\n",
       "       [0.88938809],\n",
       "       [0.67456765],\n",
       "       [0.82781424],\n",
       "       [0.54356059],\n",
       "       [0.06892322],\n",
       "       [0.10046621],\n",
       "       [0.55439198],\n",
       "       [0.32780097],\n",
       "       [0.22056807],\n",
       "       [0.46625693],\n",
       "       [0.66649674],\n",
       "       [0.38499195],\n",
       "       [0.80254226],\n",
       "       [0.73852325],\n",
       "       [0.73491488],\n",
       "       [0.98518969],\n",
       "       [0.29522998],\n",
       "       [0.64472559],\n",
       "       [0.85547834],\n",
       "       [0.57952443],\n",
       "       [0.78950218],\n",
       "       [0.41445219],\n",
       "       [0.59049135],\n",
       "       [0.05566639],\n",
       "       [0.98576761],\n",
       "       [0.80001848],\n",
       "       [0.86880907],\n",
       "       [0.69429428],\n",
       "       [0.18605003],\n",
       "       [0.03102844],\n",
       "       [0.86465661],\n",
       "       [0.39633065],\n",
       "       [0.26386488],\n",
       "       [0.56167296],\n",
       "       [0.42089852],\n",
       "       [0.651666  ],\n",
       "       [0.48379323],\n",
       "       [0.69723314],\n",
       "       [0.85342327],\n",
       "       [0.19555701],\n",
       "       [0.32943288],\n",
       "       [0.43681559],\n",
       "       [0.96205845],\n",
       "       [0.36533361],\n",
       "       [0.38109985],\n",
       "       [0.65347252],\n",
       "       [0.41919971],\n",
       "       [0.95122375],\n",
       "       [0.89446876]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "single_LSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, activation='tanh', return_sequences=False,\n",
    "                         input_shape=(5,10)),\n",
    "    tf.keras.layers.Dense(1)  \n",
    "])\n",
    "\n",
    "single_LSTM.compile(optimizer='adam', loss='mse', metrics=['mse'])  # MSE loss, MAE metric\n",
    "\n",
    " \n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "single_LSTM.compile(optimizer='adam', loss=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.5480 - mse: 0.3183\n",
      "Epoch 2/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3270 - mse: 0.1092\n",
      "Epoch 3/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2864 - mse: 0.0823\n",
      "Epoch 4/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2886 - mse: 0.0834\n",
      "Epoch 5/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2822 - mse: 0.0800\n",
      "Epoch 6/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2752 - mse: 0.0771\n",
      "Epoch 7/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2816 - mse: 0.0798\n",
      "Epoch 8/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2891 - mse: 0.0839\n",
      "Epoch 9/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2889 - mse: 0.0838\n",
      "Epoch 10/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2838 - mse: 0.0811\n",
      "Epoch 11/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2830 - mse: 0.0807\n",
      "Epoch 12/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2881 - mse: 0.0835\n",
      "Epoch 13/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2862 - mse: 0.0828\n",
      "Epoch 14/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2823 - mse: 0.0800\n",
      "Epoch 15/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2786 - mse: 0.0781\n",
      "Epoch 16/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2819 - mse: 0.0802\n",
      "Epoch 17/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2772 - mse: 0.0770\n",
      "Epoch 18/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2824 - mse: 0.0801\n",
      "Epoch 19/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2750 - mse: 0.0761\n",
      "Epoch 20/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2752 - mse: 0.0760\n",
      "Epoch 21/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2780 - mse: 0.0776 \n",
      "Epoch 22/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2751 - mse: 0.0761\n",
      "Epoch 23/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2804 - mse: 0.0791\n",
      "Epoch 24/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2815 - mse: 0.0794\n",
      "Epoch 25/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2626 - mse: 0.0701\n",
      "Epoch 26/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2798 - mse: 0.0787\n",
      "Epoch 27/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2742 - mse: 0.0758\n",
      "Epoch 28/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2825 - mse: 0.0811\n",
      "Epoch 29/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2756 - mse: 0.0764\n",
      "Epoch 30/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2756 - mse: 0.0767\n",
      "Epoch 31/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2827 - mse: 0.0804\n",
      "Epoch 32/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2714 - mse: 0.0739\n",
      "Epoch 33/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2746 - mse: 0.0760\n",
      "Epoch 34/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2753 - mse: 0.0764\n",
      "Epoch 35/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2807 - mse: 0.0792\n",
      "Epoch 36/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2782 - mse: 0.0781\n",
      "Epoch 37/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2701 - mse: 0.0737\n",
      "Epoch 38/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2787 - mse: 0.0785\n",
      "Epoch 39/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2708 - mse: 0.0737\n",
      "Epoch 40/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2798 - mse: 0.0792\n",
      "Epoch 41/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2723 - mse: 0.0749\n",
      "Epoch 42/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2690 - mse: 0.0728\n",
      "Epoch 43/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2698 - mse: 0.0734\n",
      "Epoch 44/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2840 - mse: 0.0815\n",
      "Epoch 45/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2657 - mse: 0.0716\n",
      "Epoch 46/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2670 - mse: 0.0718\n",
      "Epoch 47/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2634 - mse: 0.0705\n",
      "Epoch 48/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2697 - mse: 0.0732\n",
      "Epoch 49/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2671 - mse: 0.0722\n",
      "Epoch 50/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2710 - mse: 0.0741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ae5e213b30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_LSTM.fit(X_train,y_train , epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
